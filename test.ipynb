{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'this', 'is', 'a', 'sample', 'sentence', '[SEP]']\n",
      "Tokenize the second sentence:\n",
      "['[CLS]', 'two', 'sentences', 'are', 'form', 'a', 'paragraph', '[SEP]']\n",
      "Flatten the list:\n",
      "['[CLS]', 'this', 'is', 'a', 'sample', 'sentence', '[SEP]', '[CLS]', 'two', 'sentences', 'are', 'form', 'a', 'paragraph', '[SEP]', '[CLS]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "paragraph = \"This is a sample sentence.\\n\\nTwo sentences are form a paragraph.\"\n",
    "\n",
    "# Add the special tokens for all the sentences.\n",
    "\n",
    "sentences = paragraph.split(\".\")\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = \"[CLS] \" + sentences[i] + \" [SEP]\"\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])\n",
    "\n",
    "print (\"Tokenize the second sentence:\")\n",
    "print (tokenized_texts[1])\n",
    "\n",
    "# Now, we flatten the list.\n",
    "\n",
    "flattened_list = [y for x in tokenized_texts for y in x]\n",
    "\n",
    "print (\"Flatten the list:\")\n",
    "print (flattened_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2112, -0.0557,  0.1161,  ..., -0.0561,  0.0115,  0.3577],\n",
      "         [-0.1860, -0.0776,  0.1181,  ..., -0.0676,  0.0334,  0.3792],\n",
      "         [-0.7086, -0.4587,  0.0367,  ..., -0.2837, -0.0278,  0.5087],\n",
      "         ...,\n",
      "         [ 0.9535,  0.1016, -0.3307,  ...,  0.2683, -0.8601, -0.1683],\n",
      "         [ 0.9535,  0.1017, -0.3307,  ...,  0.2682, -0.8601, -0.1683],\n",
      "         [-0.1686, -0.2987,  0.0707,  ...,  0.0899,  0.0276,  0.4075]],\n",
      "\n",
      "        [[-0.2322,  0.0139,  0.0940,  ..., -0.0257,  0.1141,  0.2558],\n",
      "         [-0.2110,  0.0233,  0.0991,  ..., -0.0474,  0.1314,  0.2686],\n",
      "         [-0.2774, -0.3501,  0.0289,  ..., -0.1392,  0.2158, -0.0511],\n",
      "         ...,\n",
      "         [ 0.2917, -0.1940, -0.1034,  ...,  0.2925,  0.0747, -0.2758],\n",
      "         [ 0.9654,  0.2049, -0.2678,  ...,  0.2712, -0.7869, -0.1975],\n",
      "         [ 0.9654,  0.2050, -0.2677,  ...,  0.2712, -0.7869, -0.1975]],\n",
      "\n",
      "        [[-0.2519,  0.0543,  0.0957,  ..., -0.0635,  0.0188,  0.1685],\n",
      "         [-0.2274,  0.0480,  0.0948,  ..., -0.0432,  0.0786,  0.1900],\n",
      "         [ 0.7751,  0.1732, -0.1192,  ...,  0.1379, -0.6846, -0.3447],\n",
      "         ...,\n",
      "         [-0.1142,  0.0100,  0.1150,  ...,  0.2307,  0.0635,  0.3050],\n",
      "         [-0.1322, -0.0030,  0.1065,  ...,  0.2385,  0.0639,  0.3170],\n",
      "         [-0.1444,  0.0054,  0.0961,  ...,  0.2282,  0.0930,  0.2994]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Encode the sentences\n",
    "encoded_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "\n",
    "# Extract the embeddings\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Print the embeddings\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text, tokenizer, max_pos=512):\n",
    "    \"\"\"\n",
    "    - Remove \\n\n",
    "    - Sentence Tokenize\n",
    "    - Add [SEP] [CLS] as sentence boundary\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = raw_text.split(\"\\n\\n\")\n",
    "    sentences = [\"[CLS] \" + sent + \" [SEP]\" for sent in sentences]\n",
    "    paragraph = \" \".join(sentences)\n",
    "\n",
    "    return paragraph, len(sentences)\n",
    "\n",
    "def load_text(paragraph, tokenizer, max_pos=512, device=\"cpu\"):\n",
    "    sep_vid = tokenizer.vocab[\"[SEP]\"]\n",
    "    cls_vid = tokenizer.vocab[\"[CLS]\"]\n",
    "\n",
    "    paragraph = paragraph.strip().lower()\n",
    "    paragraph = paragraph.replace(\"[cls]\", \"[CLS]\")\n",
    "    paragraph = paragraph.replace(\"[sep]\", \"[SEP]\")\n",
    "\n",
    "    paragraph_subtokens = tokenizer.tokenize(paragraph)\n",
    "    paragraph_subtokens = [\"[CLS]\"] + paragraph_subtokens + [\"[SEP]\"]\n",
    "\n",
    "    print(paragraph)\n",
    "\n",
    "    subtokens_idxs = tokenizer.convert_tokens_to_ids(paragraph_subtokens)  \n",
    "\n",
    "    print(subtokens_idxs)\n",
    "\n",
    "    subtokens_idxs = subtokens_idxs[:-1][:max_pos]\n",
    "\n",
    "    subtokens_idxs[-1] = sep_vid\n",
    "\n",
    "    _segs = [-1] + [i for i, t in enumerate(subtokens_idxs) if t == sep_vid]\n",
    "    segs = [_segs[i] - _segs[i-1] for i in range(1, len(_segs))]\n",
    "\n",
    "    segments_ids = []\n",
    "    for i, s in enumerate(segs):\n",
    "        if (i % 2 == 0):\n",
    "            segments_ids += s * [0]\n",
    "        else:\n",
    "            segments_ids += s * [1]\n",
    "\n",
    "    src = torch.tensor(subtokens_idxs)[None, :].to(device)\n",
    "    mask_src = (1 - (src == 0).float()).to(device)\n",
    "    cls_ids = [[i for i, t in enumerate(subtokens_idxs) if t == cls_vid]]\n",
    "    clss = torch.tensor(cls_ids).to(device)\n",
    "    mask_cls = 1 - (clss == -1).float()\n",
    "    clss[clss == -1] = 0\n",
    "\n",
    "    segs = torch.tensor(segments_ids).to(device)\n",
    "    src_text = [[sent.replace(\"[SEP]\", \"\").strip() for sent in paragraph.split(\"[CLS]\")]]\n",
    "\n",
    "    return src, mask_src, segs, clss, mask_cls, src_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This. [SEP] [CLS] Two sentences. [SEP]\n",
      "2\n",
      "[CLS] this. [SEP] [CLS] two sentences. [SEP]\n",
      "[101, 101, 2023, 1012, 102, 101, 2048, 11746, 1012, 102, 102]\n",
      "tensor([[  101,   101,  2023,  1012,   102,   101,  2048, 11746,  1012,   102]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
      "tensor([[0, 1, 5]])\n",
      "tensor([[1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"This.\\n\\nTwo sentences.\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "paragraph, nb_sent = preprocess(paragraph, tokenizer)\n",
    "\n",
    "print(paragraph)\n",
    "print(nb_sent)\n",
    "\n",
    "src, mask_src, segs, clss, mask_cls, src_txt = load_text(paragraph, tokenizer)\n",
    "\n",
    "print(src)\n",
    "print(mask_src)\n",
    "print(segs)\n",
    "print(clss)\n",
    "print(mask_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jose/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_text = ['[CLS] ' + sent + ' [SEP]' for sent in sentences]\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] This is a sample sentence. [SEP]', '[CLS] The model is able to predict the sentiment of the sentence. [SEP]']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"This is a sample sentence. The model is able to predict the sentiment of the sentence.\"\n",
    "\n",
    "tokenized_text = preprocess(example_text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_segment_embeddings(model):\n",
    "    ea_embedding = torch.nn.Embedding(2, model.config.dim)\n",
    "    eb_embedding = torch.nn.Embedding(2, model.config.dim)\n",
    "    return ea_embedding, eb_embedding\n",
    "\n",
    "def encode_sentences(sentences, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    segment_ids = []  # Keep track of segments (odd or even)\n",
    "\n",
    "    for i, sent in enumerate(sentences):\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,\n",
    "                            add_special_tokens = False,\n",
    "                            max_length = 128,\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt',\n",
    "                       )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        segment_ids.append(torch.full((1, 128), i % 2))  # 0 for even, 1 for odd sentences\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    segment_ids = torch.cat(segment_ids, dim=0)\n",
    "    \n",
    "    return input_ids, attention_masks, segment_ids\n",
    "\n",
    "def add_segment_embeddings(embeddings, segment_ids, ea_embedding, eb_embedding):\n",
    "    # Get the batch size and sequence length from embeddings\n",
    "    batch_size, seq_length, hidden_size = embeddings.size()\n",
    "\n",
    "    # Expand segment embeddings to match the dimensions of BERT embeddings\n",
    "    ea_embeddings = ea_embedding(segment_ids).view(batch_size, seq_length, hidden_size)\n",
    "    eb_embeddings = eb_embedding(segment_ids).view(batch_size, seq_length, hidden_size)\n",
    "\n",
    "    # Add segment embeddings to the original embeddings\n",
    "    enhanced_embeddings = embeddings + torch.where(segment_ids.unsqueeze(-1) == 0, ea_embeddings, eb_embeddings)\n",
    "\n",
    "    return enhanced_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(input_ids, attention_masks, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "    return outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_embeddings(embeddings):\n",
    "    # Assuming that the first token of each sentence is [CLS]\n",
    "    return embeddings[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] This is a sample sentence. [SEP]', '[CLS] The model is able to predict the sentiment of the sentence. [SEP]']\n",
      "tensor([[ 0.1718,  1.3256, -0.1499,  ...,  0.8166,  0.3480, -0.3955],\n",
      "        [-2.4270, -0.9475, -2.0156,  ...,  0.0617,  0.4753,  0.7546]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def pipeline(text, model, tokenizer, ea_embedding, eb_embedding):\n",
    "    sentences = preprocess(text)\n",
    "    input_ids, attention_masks, segment_ids = encode_sentences(sentences, tokenizer)\n",
    "    embeddings = get_embeddings(input_ids, attention_masks, model)\n",
    "    embeddings = add_segment_embeddings(embeddings, segment_ids, ea_embedding, eb_embedding)\n",
    "    sentence_embeddings = extract_sentence_embeddings(embeddings)\n",
    "    return sentences, sentence_embeddings\n",
    "\n",
    "sentences, sentence_embeddings = pipeline(example_text, model, tokenizer, *initialize_segment_embeddings(model))\n",
    "\n",
    "print(sentences)\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2615],\n",
      "        [0.4197]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the linear layer\n",
    "linear_layer = nn.Linear(in_features=sentence_embeddings.size(1), out_features=1)\n",
    "\n",
    "# Apply the linear layer to the sentence embeddings\n",
    "classification_output = linear_layer(sentence_embeddings)\n",
    "\n",
    "# Apply sigmoid activation function\n",
    "probabilities = nn.functional.sigmoid(classification_output)\n",
    "\n",
    "# Print the probabilities\n",
    "print(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b c\n"
     ]
    }
   ],
   "source": [
    "text = 'a X b X c'\n",
    "text = text.split(' X ')[1:]\n",
    "print(\" \".join(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
